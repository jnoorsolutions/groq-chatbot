import streamlit as st                                      # ğŸ”§ Streamlit for UI
from dotenv import load_dotenv                              # ğŸ”§ load .env into os.environ
import os                                                   # ğŸ”§ interact with environment vars

from langchain_groq import ChatGroq                         # ğŸ”§ Groq LLM integration
from langchain.memory import ConversationBufferMemory       # ğŸ”§ memory backend for chat
from langchain.chains import ConversationChain              # ğŸ”§ chain that wires LLM + memory

# â”€â”€â”€ Load API Key â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()                                               # ğŸ“¥ read .env file
# os.environ["GROQ_API_KEY"] = os.getenv("GROQ_API_KEY")      # ğŸ”‘ set Groq key
os.environ["GROQ_API_KEY"] = st.secrets["GROQ_API_KEY"]

# â”€â”€â”€ Streamlit App Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="Groq Memory Chatbot")        # ğŸ–¥ï¸ title in browser tab
st.title("Groq Chatbot with Memory")                       # ğŸ–¥ï¸ app header

# â”€â”€â”€ Sidebar Controls â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model_name = st.sidebar.selectbox(                          # ğŸ”½ pick a supported model
    "Select Groq Model",
    ["llama3-8b-8192", "deepseek-r1-distill-llama-70b", "gemma2-9b-it"]
)
temperature = st.sidebar.slider(                           # ğŸ² randomness
    "Temperature", 0.0, 1.0, 0.7
)
max_tokens = st.sidebar.slider(                            # ğŸ“ max response length
    "Max Tokens", 50, 300, 150
)

# â”€â”€â”€ Initialize Memory & History â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "memory" not in st.session_state:
    # ğŸ”„ persist memory across reruns
    st.session_state.memory = ConversationBufferMemory(
        return_messages=True   # return as list of messages, not one big string
    )

if "history" not in st.session_state:
    # ğŸ—’ï¸ store role/content pairs for display
    st.session_state.history = []

# â”€â”€â”€ User Input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
user_input = st.chat_input("You:")  # â†©ï¸ widget that clears itself on Enter

if user_input:
    # ğŸ“ append user turn to visible history
    st.session_state.history.append(("user", user_input))

    # ğŸ”„ instantiate a fresh LLM for this turn
    llm = ChatGroq(
        model_name=model_name,
        temperature=temperature,
        max_tokens=max_tokens
    )

    # ğŸ”— build ConversationChain with our memory
    conv = ConversationChain(
        llm=llm,
        memory=st.session_state.memory,
        verbose=False
    )

    # ğŸ¤– get AI response (memory is updated internally)
    ai_response = conv.predict(input=user_input)

    # ğŸ“ append assistant turn to visible history
    st.session_state.history.append(("assistant", ai_response))

# â”€â”€â”€ Render Chat Bubbles â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for role, text in st.session_state.history:
    if role == "user":
        st.chat_message("user").write(text)         # ğŸ—¨ï¸ user style
    else:
        st.chat_message("assistant").write(text)    # ğŸ—¨ï¸ assistant style
